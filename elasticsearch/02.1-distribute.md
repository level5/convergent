# Distribute


## Routing a document to a shard

```
shard = hash(routing) % number_of_primary_shards
```

`routing`是任意的string。默认是使用document的`_id`,也可以设定自定义的值

这就是为什么一个index的shard数量只有在定义的时候可以指定而不能修改了。(怎么办?)

All document APIs (get, index, delete, bulk, update, and mget) accept a routing parameter that can be used to customize the document-to- shard mapping.

A custom routing value could be used to ensure that all related documents—for instance, all the documents belonging to the same user—are stored on the same shard.


```
node1(master)   node2       node3
R0              R0          P0
P1              R1          R1

```

Every node is fully capable of serving any request. Every node knows the location of every document in the cluster and so can forward requests directly to the required node.

1. The client sends a create, index, or delete request to Node 1.


2. The node uses the document’s `_id` to determine that the document belongs to shard 0. It forwards the request to Node 3, where the primary copy of shard 0 is currently allocated.
3. Node 3 executes the request on the primary shard. If it is successful, it forwards the request in parallel to the replica shards on Node 1 and Node 2. Once all of the replica shards report success, Node 3 reports success to the coordinating node, which reports success to the client.

### create

* consistency

  会要求一定数量的shard可用(primary或者replica)

  ```
    int( (primary + number_of_replicas) / 2 ) + 1
  ```

  比如定义了3个replica, `int((pirmary + 3 replicas)/2) + 1 = 3`, 如果这个时候只有两个node启动了,那么就没办法左create或者删除操作.

  the requirement for a quorum is enforced only when number_of_replicas is greater than 1.

  因为默认的配置是1 replica,如果这样,就会要求quorum是2,那么,对于单节点的服务就无法使用了,所以就有了上面这个情况.

* timeout

  当没有足够的shard copy可用的时候,elasticsearch会等待新的shard出现,默认是一分钟.

### search

1. The client sends a get request to Node 1.

2. The node uses the document’s `_id` to determine that the document belongs to shard 0. Copies of shard 0 exist on all three nodes. On this occasion, it forwards the request to Node 2.

3. Node 2 returns the document to Node 1, which returns the document to the client.

For read requests, the coordinating node will choose a different shard copy on every request in order to balance the load; it round-robins through all shard copies.

#### query phase

#### fetch phase
1. coordinating node 决定哪些document需要fetch，并且发送多个GET请求到相关的shard
2. 每一个对应的shard读取对应的documents，返回给coordinating node
3. coordinating node返回结果

### partial update

1. The client sends an update request to Node 1.

2. It forwards the request to Node 3, where the primary shard is allocated.

3. Node 3 retrieves the document from the primary shard, changes the JSON in the `_source` field, and tries to reindex the document on the primary shard. If the document has already been changed by another process, it retries step 3 up to retry_on_conflict times, before giving up.

4. If Node 3 has managed to update the document successfully, it forwards the new version of the document in parallel to the replica shards on Node 1 and Node 2 to be reindexed. Once all replica shards report success, Node 3 reports success to the coordinating node, which reports success to the client.


When a primary shard forwards changes to its replica shards, it doesn’t forward the update request. Instead it forwards the new version of the full document.

原因:
Remember that these changes are forwarded to the replica shards asynchronously, and there is no guarantee that they will arrive in the same order that they were sent. If Elasticsearch forwarded just the change, it is possible that changes would be applied in the wrong order, resulting in a corrupt document.

### Multidocument Patterns

`mget`, `bulk`操作

`mget`

1. The client sends an mget request to Node 1.

2. Node 1 builds a multi-get request per shard, and forwards these requests in parallel to the nodes hosting each required primary or replica shard. Once all replies have been received, Node 1 builds the response and returns it to the client.

`bulk`
1. The client sends a bulk request to Node 1.

2. Node 1 builds a bulk request per shard, and forwards these requests in parallel to the nodes hosting each involved primary shard.

3. The primary shard executes each action serially, one after another. As each action succeeds, the primary forwards the new document (or deletion) to its replica shards in parallel, and then moves on to the next action. Once all replica shards report success for all actions, the node reports success to the coordinating node, which collates the responses and returns them to the client.
